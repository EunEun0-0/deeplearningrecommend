{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d33805",
   "metadata": {},
   "source": [
    "성능을 올리기 위해  epoch 5-> 20으로 증가시켜봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57b3aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\Desktop\\VE_MQ\\autoint\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, MaxPooling2D, Conv2D, Dropout, Lambda, Dense, Flatten, Activation, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_normal, Zeros, TruncatedNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb10035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(Layer):\n",
    "    '''임베딩 레이어입니다.'''\n",
    "    def __init__(self, field_dims, embed_dim, **kwargs):\n",
    "        super(FeaturesEmbedding, self).__init__(**kwargs)\n",
    "        self.total_dim = sum(field_dims)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.longlong)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.total_dim, output_dim=self.embed_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding.build(input_shape)\n",
    "        self.embedding.set_weights([tf.keras.initializers.GlorotUniform()(shape=self.embedding.weights[0].shape)])\n",
    "\n",
    "    # ✅ 들여쓰기 오류 수정\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, dtype=tf.int64)\n",
    "        x = x + tf.constant(self.offsets)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091a8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(Layer):  \n",
    "    '''\n",
    "    DNN 레이어입니다.\n",
    "    - Tensorflow Keras에서는 Dense 레이어를 쌓아올린 구조입니다.\n",
    "    - 필요에 따라 배치 정규화도 사용할 수 있습니다.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, init_std=0.0001, output_layer=True):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        hidden_units = [input_dim] + list(hidden_units)\n",
    "        if output_layer:\n",
    "            hidden_units += [1]\n",
    "        # Dense layer를 쌓아올립니다.\n",
    "        self.linears = [Dense(units, activation=None, kernel_initializer=tf.random_normal_initializer(stddev=init_std),\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)) for units in hidden_units[1:]]\n",
    "        # 활성화 함수를 세팅합니다.\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        # 필요하다면 배치정규화도 진행합니다.\n",
    "        if self.use_bn:\n",
    "            self.bn = [BatchNormalization() for _ in hidden_units[1:]]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for i in range(len(self.linears)):\n",
    "            # input data가 들어오면 layer를 돌면서 벡터 값을 가져오게 됩니다.\n",
    "            x = self.linears[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn[i](x, training=training)\n",
    "            # 각 layer마다 나온 벡터 값에 활성화 함수와 dropout을 적용시켜 비선형성 구조와 과적합을 방지합니다.\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620de536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer):  \n",
    "    '''\n",
    "    멀티 헤드 셀프 어텐션 레이어입니다.\n",
    "    - 위에 작성한 수식과 같이 동작됩니다.\n",
    "    - 필요에 따라 잔차 연결(residual connection)도 진행합니다.\n",
    "    '''\n",
    "    def __init__(self, att_embedding_size=8, head_num=2, use_res=True, scaling=False, seed=1024, **kwargs):\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        self.att_embedding_size = att_embedding_size\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.seed = seed\n",
    "        self.scaling = scaling\n",
    "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(input_shape)))\n",
    "        embedding_size = int(input_shape[-1])\n",
    "        # 쿼리에 해당하는 매트릭스입니다. \n",
    "        self.W_Query = self.add_weight(name='query', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed))\n",
    "        # 키에 해당되는 매트릭스입니다.\n",
    "        self.W_key = self.add_weight(name='key', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=TruncatedNormal(seed=self.seed + 1))\n",
    "        # 값(value)에 해당되는 매트릭스입니다.\n",
    "        self.W_Value = self.add_weight(name='value', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed + 2))\n",
    "        # 필요하다면 잔차 연결도 할 수 있습니다.\n",
    "        if self.use_res:\n",
    "            self.W_Res = self.add_weight(name='res', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=TruncatedNormal(seed=self.seed))\n",
    "\n",
    "        super(MultiHeadSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if K.ndim(inputs) != 3:\n",
    "            raise ValueError(\"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (K.ndim(inputs)))\n",
    "        \n",
    "        # 입력이 들어오면 쿼리, 키, 값(value)에 매칭되어 각각의 값을 가지고 옵니다.\n",
    "        querys = tf.tensordot(inputs, self.W_Query, axes=(-1, 0))  \n",
    "        keys = tf.tensordot(inputs, self.W_key, axes=(-1, 0))\n",
    "        values = tf.tensordot(inputs, self.W_Value, axes=(-1, 0))\n",
    "\n",
    "        # 헤드 개수에 따라 데이터를 분리해줍니다.\n",
    "        querys = tf.stack(tf.split(querys, self.head_num, axis=2))\n",
    "        keys = tf.stack(tf.split(keys, self.head_num, axis=2))\n",
    "        values = tf.stack(tf.split(values, self.head_num, axis=2))\n",
    "        \n",
    "        # 쿼리와 키를 먼저 곱해줍니다. 위 이미지의 식 (5)와 같습니다.\n",
    "        inner_product = tf.matmul(querys, keys, transpose_b=True)\n",
    "        if self.scaling:\n",
    "            inner_product /= self.att_embedding_size ** 0.5\n",
    "        self.normalized_att_scores =  tf.nn.softmax(inner_product)\n",
    "        \n",
    "        # 쿼리와 키에서 나온 어텐션 값을 값(value)에 곱해줍니다. 식 (6)과 같습니다.\n",
    "        result = tf.matmul(self.normalized_att_scores, values)\n",
    "        # 식 (7)과 같이 쪼개어진 멀테 헤드를 모아줍니다.\n",
    "        result = tf.concat(tf.split(result, self.head_num, ), axis=-1)\n",
    "        result = tf.squeeze(result, axis=0) \n",
    "\n",
    "        if self.use_res:\n",
    "            result += tf.tensordot(inputs, self.W_Res, axes=(-1, 0))\n",
    "        result = tf.nn.relu(result)\n",
    "        \n",
    "        # 그 결과 값을 리턴합니다.\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (None, input_shape[1], self.att_embedding_size * self.head_num)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res,'seed': self.seed}\n",
    "        base_config = super(MultiHeadSelfAttention, self).get_config()\n",
    "        base_config.update(config)\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea3afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIntMLP(Layer):\n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, dnn_dropout=0.4, init_std=0.0001):\n",
    "        super(AutoIntMLP, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embedding_size)\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.final_layer = Dense(1, use_bias=False, kernel_initializer=tf.random_normal_initializer(stddev=init_std))\n",
    "        \n",
    "        self.dnn = tf.keras.Sequential()\n",
    "        for units in dnn_hidden_units:\n",
    "            self.dnn.add(Dense(units, activation=dnn_activation,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l2_reg_dnn),\n",
    "                               kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "            if dnn_use_bn:\n",
    "                self.dnn.add(BatchNormalization())\n",
    "            # ✅ 중복 활성화 함수 제거 (Dense 레이어에 이미 포함됨)\n",
    "            # self.dnn.add(Activation(dnn_activation)) \n",
    "            if dnn_dropout > 0:\n",
    "                self.dnn.add(Dropout(dnn_dropout))\n",
    "        self.dnn.add(Dense(1, kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "\n",
    "        self.int_layers = [MultiHeadSelfAttention(att_embedding_size=embedding_size, head_num=att_head_num, use_res=att_res) for _ in range(att_layer_num)]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dnn_input_dim = self.embedding_size * self.num_fields\n",
    "        self.dnn.build(input_shape=(None, dnn_input_dim))\n",
    "        super(AutoIntMLP, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embed_x = self.embedding(inputs)\n",
    "        \n",
    "        # 어텐션 경로\n",
    "        att_input = tf.reshape(embed_x, shape=(-1, self.num_fields, self.embedding_size))\n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "        att_output = Flatten()(att_input)\n",
    "        att_output = self.final_layer(att_output)\n",
    "        \n",
    "        # DNN 경로\n",
    "        dnn_embed = tf.reshape(embed_x, shape=(-1, self.embedding_size * self.num_fields))\n",
    "        dnn_output = self.dnn(dnn_embed)\n",
    "\n",
    "        y_pred = tf.keras.activations.sigmoid(att_output + dnn_output)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce75dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIntMLPModel(Model):\n",
    "    def __init__(self, field_dims, embedding_size, **kwargs):\n",
    "        super(AutoIntMLPModel, self).__init__()\n",
    "        self.auto_int_mlp = AutoIntMLP(field_dims=field_dims, embedding_size=embedding_size, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.auto_int_mlp(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3a913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da5597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# 손실 함수: 모델의 최종 출력이 sigmoid이므로 BinaryCrossentropy 사용\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062477c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6421566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCG(ranklist, y_true):\n",
    "    dcg = 0.0\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item in y_true:\n",
    "            dcg += 1.0 / math.log(i + 2)\n",
    "    return  dcg\n",
    "\n",
    "def get_IDCG(ranklist, y_true):\n",
    "    idcg = 0.0\n",
    "    i = 0\n",
    "    for item in y_true:\n",
    "        if item in ranklist:\n",
    "            idcg += 1.0 / math.log(i + 2)\n",
    "            i += 1\n",
    "    return idcg\n",
    "\n",
    "def get_NDCG(ranklist, y_true):\n",
    "    '''NDCG 평가 지표'''\n",
    "    ranklist = np.array(ranklist).astype(int)\n",
    "    y_true = np.array(y_true).astype(int)\n",
    "    dcg = get_DCG(ranklist, y_true)\n",
    "    idcg = get_IDCG(y_true, y_true)\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    return round( (dcg / idcg), 5)\n",
    "\n",
    "def get_hit_rate(ranklist, y_true):\n",
    "    '''hitrate 평가 지표'''\n",
    "    c = 0\n",
    "    for y in y_true:\n",
    "        if y in ranklist:\n",
    "            c += 1\n",
    "    return round( c / len(y_true), 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce98eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_df):\n",
    "    '''모델 테스트'''\n",
    "    user_pred_info = defaultdict(list)\n",
    "    total_rows = len(test_df)\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        features = test_df.iloc[i:i + batch_size, :-1].values\n",
    "        y_pred = model.predict(features, verbose=False)\n",
    "        for feature, p in zip(features, y_pred):\n",
    "            u_i = feature[:2]\n",
    "            user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
    "    return user_pred_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae489b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"\\\\Users\\\\USER\\\\Desktop\\\\VE_MQ\\\\autoint\"\n",
    "data_dir_nm = 'data'\n",
    "movielens_dir_nm = 'ml-1m'\n",
    "model_dir_nm = 'model'\n",
    "data_path = f\"{project_path}\\\\{data_dir_nm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93345903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_decade</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>rating_month</th>\n",
       "      <th>rating_decade</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1970s</td>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>1990s</td>\n",
       "      <td>1996</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>1960s</td>\n",
       "      <td>1964</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Musical</td>\n",
       "      <td>Romance</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>2000s</td>\n",
       "      <td>2000</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>1990s</td>\n",
       "      <td>1998</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id movie_id movie_decade movie_year rating_year rating_month  \\\n",
       "0       1     1193        1970s       1975        2001            1   \n",
       "1       1      661        1990s       1996        2001            1   \n",
       "2       1      914        1960s       1964        2001            1   \n",
       "3       1     3408        2000s       2000        2001            1   \n",
       "4       1     2355        1990s       1998        2001            1   \n",
       "\n",
       "  rating_decade     genre1      genre2   genre3 gender age occupation    zip  \\\n",
       "0         2000s      Drama          no       no      F   1         10  48067   \n",
       "1         2000s  Animation  Children's  Musical      F   1         10  48067   \n",
       "2         2000s    Musical     Romance       no      F   1         10  48067   \n",
       "3         2000s      Drama          no       no      F   1         10  48067   \n",
       "4         2000s  Animation  Children's   Comedy      F   1         10  48067   \n",
       "\n",
       "  label  \n",
       "0     1  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens_rcmm = pd.read_csv(f\"{data_path}\\\\ml-1m\\\\movielens_rcmm_v2.csv\", dtype=str)\n",
    "print(movielens_rcmm.shape)\n",
    "movielens_rcmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3642c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {col: LabelEncoder() for col in movielens_rcmm.columns[:-1]} # label은 제외\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    movielens_rcmm[col] = le.fit_transform(movielens_rcmm[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d269b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_rcmm['label'] = movielens_rcmm['label'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4178952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(movielens_rcmm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c9014c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6040, 3706,   10,   81,    4,   12,    1,   18,   18,   16,    2,\n",
       "          7,   21, 3439], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_i_feature = ['user_id', 'movie_id']\n",
    "meta_features = ['movie_decade', 'movie_year', 'rating_year', 'rating_month', 'rating_decade', 'genre1','genre2', 'genre3', 'gender', 'age', 'occupation', 'zip']\n",
    "label = 'label'\n",
    "features = u_i_feature + meta_features\n",
    "field_dims = np.max(movielens_rcmm[u_i_feature + meta_features].astype(np.int64).values, axis=0) + 1\n",
    "field_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace35679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_auc', mode='max', patience=3, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be35053",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "learning_rate= 0.001\n",
    "dropout= 0.4\n",
    "batch_size = 2048\n",
    "embed_dim= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3530086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\Desktop\\VE_MQ\\autoint\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoIntMLP_model = AutoIntMLPModel(field_dims=field_dims, embedding_size=embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d82948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoIntMLP_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ceed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\USER\\Desktop\\VE_MQ\\autoint\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\USER\\Desktop\\VE_MQ\\autoint\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "352/352 [==============================] - 90s 231ms/step - loss: 0.5895 - accuracy: 0.6768 - auc: 0.7394 - val_loss: 0.5413 - val_accuracy: 0.7272 - val_auc: 0.7905\n",
      "Epoch 2/20\n",
      "352/352 [==============================] - 85s 241ms/step - loss: 0.5363 - accuracy: 0.7288 - auc: 0.7952 - val_loss: 0.5339 - val_accuracy: 0.7308 - val_auc: 0.7960\n",
      "Epoch 3/20\n",
      "352/352 [==============================] - 85s 242ms/step - loss: 0.5278 - accuracy: 0.7330 - auc: 0.8020 - val_loss: 0.5307 - val_accuracy: 0.7336 - val_auc: 0.7988\n",
      "Epoch 4/20\n",
      "352/352 [==============================] - 84s 239ms/step - loss: 0.5227 - accuracy: 0.7352 - auc: 0.8062 - val_loss: 0.5299 - val_accuracy: 0.7341 - val_auc: 0.8002\n",
      "Epoch 5/20\n",
      "352/352 [==============================] - 85s 241ms/step - loss: 0.5194 - accuracy: 0.7368 - auc: 0.8091 - val_loss: 0.5288 - val_accuracy: 0.7340 - val_auc: 0.8015\n",
      "Epoch 6/20\n",
      "352/352 [==============================] - 72s 205ms/step - loss: 0.5167 - accuracy: 0.7389 - auc: 0.8115 - val_loss: 0.5278 - val_accuracy: 0.7342 - val_auc: 0.8026\n",
      "Epoch 7/20\n",
      "352/352 [==============================] - 55s 155ms/step - loss: 0.5141 - accuracy: 0.7403 - auc: 0.8138 - val_loss: 0.5263 - val_accuracy: 0.7358 - val_auc: 0.8039\n",
      "Epoch 8/20\n",
      "352/352 [==============================] - 56s 159ms/step - loss: 0.5113 - accuracy: 0.7423 - auc: 0.8164 - val_loss: 0.5243 - val_accuracy: 0.7371 - val_auc: 0.8053\n",
      "Epoch 9/20\n",
      "352/352 [==============================] - 57s 162ms/step - loss: 0.5081 - accuracy: 0.7446 - auc: 0.8191 - val_loss: 0.5231 - val_accuracy: 0.7384 - val_auc: 0.8068\n",
      "Epoch 10/20\n",
      "352/352 [==============================] - 57s 162ms/step - loss: 0.5044 - accuracy: 0.7474 - auc: 0.8222 - val_loss: 0.5229 - val_accuracy: 0.7385 - val_auc: 0.8084\n",
      "Epoch 11/20\n",
      "352/352 [==============================] - 62s 177ms/step - loss: 0.4998 - accuracy: 0.7498 - auc: 0.8259 - val_loss: 0.5209 - val_accuracy: 0.7389 - val_auc: 0.8096\n",
      "Epoch 12/20\n",
      "352/352 [==============================] - 87s 247ms/step - loss: 0.4950 - accuracy: 0.7528 - auc: 0.8298 - val_loss: 0.5199 - val_accuracy: 0.7414 - val_auc: 0.8109\n",
      "Epoch 13/20\n",
      "352/352 [==============================] - 83s 236ms/step - loss: 0.4905 - accuracy: 0.7558 - auc: 0.8333 - val_loss: 0.5199 - val_accuracy: 0.7419 - val_auc: 0.8118\n",
      "Epoch 14/20\n",
      "352/352 [==============================] - 83s 235ms/step - loss: 0.4860 - accuracy: 0.7585 - auc: 0.8368 - val_loss: 0.5219 - val_accuracy: 0.7422 - val_auc: 0.8117\n",
      "Epoch 15/20\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 0.4815 - accuracy: 0.7613 - auc: 0.8402 - val_loss: 0.5225 - val_accuracy: 0.7417 - val_auc: 0.8122\n",
      "Epoch 16/20\n",
      "352/352 [==============================] - 81s 230ms/step - loss: 0.4772 - accuracy: 0.7640 - auc: 0.8436 - val_loss: 0.5219 - val_accuracy: 0.7419 - val_auc: 0.8121\n",
      "Epoch 17/20\n",
      "352/352 [==============================] - 70s 198ms/step - loss: 0.4724 - accuracy: 0.7671 - auc: 0.8471 - val_loss: 0.5239 - val_accuracy: 0.7414 - val_auc: 0.8113\n",
      "Epoch 18/20\n",
      "352/352 [==============================] - 82s 234ms/step - loss: 0.4681 - accuracy: 0.7698 - auc: 0.8503 - val_loss: 0.5264 - val_accuracy: 0.7419 - val_auc: 0.8111\n",
      "Epoch 19/20\n",
      "352/352 [==============================] - 87s 246ms/step - loss: 0.4636 - accuracy: 0.7729 - auc: 0.8536 - val_loss: 0.5303 - val_accuracy: 0.7409 - val_auc: 0.8105\n",
      "Epoch 20/20\n",
      "352/352 [==============================] - 79s 223ms/step - loss: 0.4589 - accuracy: 0.7757 - auc: 0.8570 - val_loss: 0.5327 - val_accuracy: 0.7417 - val_auc: 0.8101\n"
     ]
    }
   ],
   "source": [
    "history = autoIntMLP_model.fit(\n",
    "    train_df[features].astype(np.float32), \n",
    "    train_df[label], \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ea71c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23780\\2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|██████████| 6038/6038 [00:00<00:00, 35702.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# 사용자에게 예측된 정보를 저장하는 딕셔너리 \n",
    "user_pred_info = {}\n",
    "# top10개\n",
    "top = 10\n",
    "# 테스트 값을 가지고 옵니다. \n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df)\n",
    "# 사용자마다 돌면서 예측 데이터 중 가장 높은 top 10만 가져옵니다. \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "# 원본 테스트 데이터에서 label이 1인 사용자 별 영화 정보를 가져옵니다.\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee5d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5994/5994 [00:01<00:00, 3733.03it/s]\n",
      "100%|██████████| 5994/5994 [00:00<00:00, 32721.77it/s]\n"
     ]
    }
   ],
   "source": [
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "# 모델 예측값과 원본 테스트 데이터를 비교해서 어느정도 성능이 나왔는지 NDCG와 Hitrate를 비교합니다.\n",
    "\n",
    "# NDCG\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # NDCG 값 구하기\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "# Hitrate\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # hitrate 값 구하기\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    # 사용자 hitrate 결과 저장\n",
    "    mymodel_hitrate_result[user] = user_hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c02a50ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.66678\n",
      " mymodel hitrate :  0.6357\n"
     ]
    }
   ],
   "source": [
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc3bad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('\\\\Users\\\\USER\\\\Desktop\\\\VE_MQ\\\\autoint\\\\field_dims.npy', field_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cea9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoIntMLP_model.save_weights('\\\\Users\\\\USER\\\\Desktop\\\\VE_MQ\\\\autoint\\\\model\\\\autoIntMLP_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f86f7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib \n",
    "\n",
    "# # 모델 객체를 pickled binary file 형태로 저장\n",
    "# joblib.dump(label_encoders, '\\\\Users\\\\USER\\\\Desktop\\\\VE_MQ\\\\autoint\\\\label_encoders.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b8492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
